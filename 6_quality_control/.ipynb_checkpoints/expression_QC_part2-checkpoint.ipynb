{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expression Quality Control (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a template notebook for performing the final quality control on your organism's expression data. This requires a curated metadata sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from os import path\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logTPM_file = path.join('..','data','raw_data','/Users/louxuwen/Desktop/Documents/GitHub/BENG212_S_aureus/2_process_data/log_tpm.csv') # Enter log-TPM filename here\n",
    "all_metadata_file = path.join('..','data','interim','/Users/louxuwen/Desktop/Documents/GitHub/BENG212_S_aureus/3_quality control/NCTC8325_all.tsv') # Enter full metadata filename here\n",
    "metadata_file = path.join('..','data','interim','/Users/louxuwen/Desktop/Documents/GitHub/BENG212_S_aureus/3_quality control/NCTC8325_part1_curated.tsv') # Enter curated metadata filename here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_log_tpm = pd.read_csv(logTPM_file,index_col=0).fillna(0)\n",
    "print('Number of genes:',DF_log_tpm.shape[0])\n",
    "print('Number of samples:',DF_log_tpm.shape[1])\n",
    "DF_log_tpm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_metadata = pd.read_csv(metadata_file,index_col=0,sep='\\t')\n",
    "print('Number of samples with curated metadata:',DF_metadata.shape[0])\n",
    "DF_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_metadata_all = pd.read_csv(all_metadata_file,index_col=0,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove samples due to poor metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After curation, some samples either did not have enough replicates or metadata to warrant inclusion in this database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_metadata_passed_step4 = DF_metadata[~DF_metadata.skip.fillna(False)].copy()\n",
    "print('New number of samples with curated metadata:',DF_metadata_passed_step4.shape[0])\n",
    "DF_metadata_passed_step4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check curation\n",
    "Since manual curation is error-prone, we want to make sure that all samples have labels for their project and condition. In addition, there should only be one reference condition in each project, and it should be in the project itself.\n",
    "\n",
    "Any samples that fail these checks will be printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(DF_metadata_passed_step4.project.notnull().all())\n",
    "assert(DF_metadata_passed_step4.condition.notnull().all())\n",
    "\n",
    "for name,group in DF_metadata_passed_step4.groupby('project'):\n",
    "    ref_cond = group.reference_condition.unique()\n",
    "    \n",
    "    # Ensure that there is only one reference condition per project\n",
    "    if not len(ref_cond) == 1:\n",
    "        print('Multiple reference conditions for:, name')\n",
    "    \n",
    "    # Ensure the reference condition is in fact in the project\n",
    "    ref_cond = ref_cond[0]\n",
    "    if not ref_cond in group.condition.tolist():\n",
    "        print('Reference condition not in project:', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, make a new column called ``full_name`` that gives every experimental condition a unique, human-readable identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_metadata_passed_step4['full_name'] = DF_metadata_passed_step4['project'].str.cat(DF_metadata_passed_step4['condition'],sep=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove samples with only one replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, find sample names that have at least two replicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = DF_metadata_passed_step4.full_name.value_counts()\n",
    "keep_samples = counts[counts >= 2].index\n",
    "print(keep_samples[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep these samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_metadata_passed_step4 = DF_metadata_passed_step4[DF_metadata_passed_step4.full_name.isin(keep_samples)]\n",
    "print('New number of samples with curated metadata:',DF_metadata_passed_step4.shape[0])\n",
    "DF_metadata_passed_step4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save this information to the full metadata dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_metadata_all['passed_curation'] = DF_metadata_all.index.isin(DF_metadata_passed_step4.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check correlations between replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove failed data from log_tpm files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_log_tpm = DF_log_tpm[DF_metadata_passed_step4.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Pearson R Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biological replicates should have a Pearson R correlation above 0.95. For samples with more than 2 replicates, the replicates must have R >= 0.95 with at least one other replicate or it will be dropped. The correlation threshold can be changed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcutoff = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes correlations between all samples and collects correlations between replicates and non-replicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_corrs = {}\n",
    "rand_corrs = {}\n",
    "\n",
    "num_comparisons = len(DF_metadata_passed_step4)*(len(DF_metadata_passed_step4)-1)/2\n",
    "\n",
    "for exp1,exp2 in tqdm(itertools.combinations(DF_metadata_passed_step4.index,2),total=num_comparisons):\n",
    "    if DF_metadata_passed_step4.loc[exp1,'full_name'] == DF_metadata_passed_step4.loc[exp2,'full_name']:\n",
    "        rep_corrs[(exp1,exp2)] = stats.pearsonr(DF_log_tpm[exp1],DF_log_tpm[exp2])[0]\n",
    "    else:\n",
    "        rand_corrs[(exp1,exp2)] = stats.pearsonr(DF_log_tpm[exp1],DF_log_tpm[exp2])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations can be plotted on a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(5,5))\n",
    "ax2 = ax.twinx()\n",
    "ax2.hist(rep_corrs.values(),bins=50,range=(0.2,1),alpha=0.8,color='green',linewidth=0)\n",
    "ax.hist(rand_corrs.values(),bins=50,range=(0.2,1),alpha=0.8,color='blue',linewidth=0)\n",
    "ax.set_title('Pearson R correlation between experiments',fontsize=14)\n",
    "ax.set_xlabel('Pearson R correlation',fontsize=14)\n",
    "ax.set_ylabel('Different Conditions',fontsize=14)\n",
    "ax2.set_ylabel('Known Replicates',fontsize=14)\n",
    "\n",
    "med_corr = np.median([v for k,v in rep_corrs.items()])\n",
    "print('Median Pearson R between replicates: {:.2f}'.format(med_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove samples without any high-correlation replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilar = []\n",
    "for idx, grp in DF_metadata_passed_step4.groupby('full_name'):\n",
    "    ident = np.identity(len(grp))\n",
    "    corrs = (DF_log_tpm[grp.index].corr() - ident).max()\n",
    "    dissimilar.extend(corrs[corrs<rcutoff].index)\n",
    "\n",
    "# Save this information in both the original metadata dataframe and the new metadata dataframe\n",
    "DF_metadata_all['passed_replicate_correlations'] = ~DF_metadata_all.index.isin(dissimilar)\n",
    "DF_metadata_passed_step4['passed_replicate_correlations'] = ~DF_metadata_passed_step4.index.isin(dissimilar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_metadata_final = DF_metadata_passed_step4[DF_metadata_passed_step4['passed_replicate_correlations']]\n",
    "print('# Samples that passed replicate correlations:',len(DF_metadata_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that reference conditions still exist\n",
    "If a reference condition was removed due to poor replicate correlations, a new reference condition needs to be defined.\n",
    "\n",
    "Again, any samples that fail these checks will be printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_exprs = []\n",
    "for name,group in DF_metadata_final.groupby('project'):\n",
    "    \n",
    "    # Get reference condition\n",
    "    ref_cond = group.reference_condition.iloc[0]\n",
    "    \n",
    "    # Ensure the reference condition is still in the project\n",
    "    if ref_cond not in group.condition.tolist():\n",
    "        print('Reference condition missing from:', name)\n",
    "    \n",
    "    # Check that each project has at least two conditions (a reference and at least one test condition)\n",
    "    if len(group.condition.unique()) <= 1:\n",
    "        print('Only one condition in:', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, choose a new condition for failed projects and re-run notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize dataset to reference conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_metadata_final = DF_metadata_passed_step4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_log_tpm_final = DF_log_tpm[DF_metadata_final.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_exprs = []\n",
    "for name,group in DF_metadata_final.groupby('project'):\n",
    "    \n",
    "    # Get reference condition\n",
    "    ref_cond = group.reference_condition.iloc[0]\n",
    "    \n",
    "    # Get reference condition sample ids\n",
    "    ref_samples = group[group.condition == ref_cond].index\n",
    "    \n",
    "    # Get reference condition expression\n",
    "    ref_expr = DF_log_tpm_final[ref_samples].mean(axis=1)\n",
    "    \n",
    "    # Subtract reference expression from project\n",
    "    project_exprs.append(DF_log_tpm_final[group.index].sub(ref_expr,axis=0))\n",
    "\n",
    "DF_log_tpm_norm = pd.concat(project_exprs,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logTPM_qc_file = path.join('..','data','processed_data','/Users/louxuwen/Desktop/Documents/GitHub/BENG212_S_aureus/3_quality control/log_tpm.csv')\n",
    "logTPM_norm_file = path.join('..','data','processed_data','/Users/louxuwen/Desktop/Documents/GitHub/BENG212_S_aureus/3_quality control/log_tpm_norm.csv')\n",
    "final_metadata_file = path.join('..','data','processed_data','/Users/louxuwen/Desktop/Documents/GitHub/BENG212_S_aureus/3_quality control/metadata.tsv')\n",
    "final_metadata_all_file = path.join('..','data','interim','/Users/louxuwen/Desktop/Documents/GitHub/BENG212_S_aureus/3_quality control/metadata_qc_part2_all.tsv')\n",
    "\n",
    "DF_log_tpm_final.to_csv(logTPM_qc_file)\n",
    "DF_log_tpm_norm.to_csv(logTPM_norm_file)\n",
    "DF_metadata_final.to_csv(final_metadata_file, sep='\\t')\n",
    "DF_metadata_all.to_csv(final_metadata_all_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
